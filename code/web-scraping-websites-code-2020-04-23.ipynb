{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![UKDS Logo](./images/UKDS_Logos_Col_Grey_300dpi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Web-scraping for Social Science Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Welcome to the <a href=\"https://ukdataservice.ac.uk/\" target=_blank>UK Data Service</a> training series on *New Forms of Data for Social Science Research*. This series guides you through some of the most common and valuable new sources of data available for social science research: data collected from websites, social media platorms, text data, conducting simulations (agent based modelling), to name a few. To help you get to grips with these new forms of data, we provide webinars, interactive notebooks containing live programming code, reading lists and more.\n",
    "\n",
    "* To access training materials for the entire series: <a href=\"https://github.com/UKDataServiceOpen/new-forms-of-data\" target=_blank>[Training Materials]</a>\n",
    "\n",
    "* To keep up to date with upcoming and past training events: <a href=\"https://ukdataservice.ac.uk/news-and-events/events\" target=_blank>[Events]</a>\n",
    "\n",
    "* To get in contact with feedback, ideas or to seek assistance: <a href=\"https://ukdataservice.ac.uk/help.aspx\" target=_blank>[Help]</a>\n",
    "\n",
    "<a href=\"https://www.research.manchester.ac.uk/portal/julia.kasmire.html\" target=_blank>Dr Julia Kasmire</a> and <a href=\"https://www.research.manchester.ac.uk/portal/diarmuid.mcdonnell.html\" target=_blank>Dr Diarmuid McDonnell</a> <br />\n",
    "UK Data Service  <br />\n",
    "University of Manchester <br />\n",
    "April 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Guide-to-using-this-resource\" data-toc-modified-id=\"Guide-to-using-this-resource-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Guide to using this resource</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interaction\" data-toc-modified-id=\"Interaction-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Interaction</a></span></li><li><span><a href=\"#Learn-more\" data-toc-modified-id=\"Learn-more-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Learn more</a></span></li></ul></li><li><span><a href=\"#Collecting-data-from-web-pages\" data-toc-modified-id=\"Collecting-data-from-web-pages-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Collecting data from web pages</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-web-scraping?\" data-toc-modified-id=\"What-is-web-scraping?-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>What is web-scraping?</a></span></li><li><span><a href=\"#Reasons-to-engage-in-web-scraping\" data-toc-modified-id=\"Reasons-to-engage-in-web-scraping-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Reasons to engage in web-scraping</a></span></li><li><span><a href=\"#Logic-of-web-scraping\" data-toc-modified-id=\"Logic-of-web-scraping-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Logic of web-scraping</a></span></li></ul></li><li><span><a href=\"#Example:-Capturing-Covid-19-data\" data-toc-modified-id=\"Example:-Capturing-Covid-19-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Example: Capturing Covid-19 data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identifying-the-web-address\" data-toc-modified-id=\"Identifying-the-web-address-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Identifying the web address</a></span></li><li><span><a href=\"#Locating-information\" data-toc-modified-id=\"Locating-information-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Locating information</a></span></li><li><span><a href=\"#Requesting-the-web-page\" data-toc-modified-id=\"Requesting-the-web-page-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Requesting the web page</a></span></li><li><span><a href=\"#Parsing-the-web-page\" data-toc-modified-id=\"Parsing-the-web-page-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Parsing the web page</a></span></li><li><span><a href=\"#Extracting-information\" data-toc-modified-id=\"Extracting-information-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Extracting information</a></span></li><li><span><a href=\"#Saving-results-from-the-scrape\" data-toc-modified-id=\"Saving-results-from-the-scrape-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Saving results from the scrape</a></span></li><li><span><a href=\"#Country-level-Covid-19-data\" data-toc-modified-id=\"Country-level-Covid-19-data-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Country-level Covid-19 data</a></span></li><li><span><a href=\"#Concluding-remarks-on-Covid-19-data\" data-toc-modified-id=\"Concluding-remarks-on-Covid-19-data-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Concluding remarks on Covid-19 data</a></span></li><li><span><a href=\"#A-social-research-example:-charity-data\" data-toc-modified-id=\"A-social-research-example:-charity-data-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>A social research example: charity data</a></span></li></ul></li><li><span><a href=\"#Value,-limitations-and-ethics\" data-toc-modified-id=\"Value,-limitations-and-ethics-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Value, limitations and ethics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Value\" data-toc-modified-id=\"Value-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Value</a></span></li><li><span><a href=\"#Limitations\" data-toc-modified-id=\"Limitations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Limitations</a></span></li><li><span><a href=\"#Ethical-considerations\" data-toc-modified-id=\"Ethical-considerations-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Ethical considerations</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Bibliography\" data-toc-modified-id=\"Bibliography-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Bibliography</a></span></li><li><span><a href=\"#Further-reading-and-resources\" data-toc-modified-id=\"Further-reading-and-resources-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Further reading and resources</a></span></li><li><span><a href=\"#Appendices\" data-toc-modified-id=\"Appendices-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Appendices</a></span><ul class=\"toc-item\"><li><span><a href=\"#Appendix-A---Requesting-URLs\" data-toc-modified-id=\"Appendix-A---Requesting-URLs-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Appendix A - Requesting URLs</a></span></li><li><span><a href=\"#Appendix-B---Capturing-charity-data\" data-toc-modified-id=\"Appendix-B---Capturing-charity-data-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Appendix B - Capturing charity data</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this training series we cover some of the essential skills needed to collect data from the web. In particular we focus on two different approaches:\n",
    "1. Collecting data stored on web pages. [Focus of this notebook]\n",
    "2. Downloading data from online databases using Application Programming Interfaces (APIs). <a href=\"./web-scraping-apis-code-2020-04-30.ipynb\" target=_blank>[LINK]</a>\n",
    "    \n",
    "Do not be alarmed by the technical aspects: both approaches can be implemented using simple code, a standard desktop or laptop, and a decent internet connection.    \n",
    "\n",
    "Given the Covid-19 public health crisis in which this programme of work occurred, we will examine ways in which computational methods can provide valuable data for studying this phenomenon. This is a fast moving, evolving public health emergency that, in addition to other impacts, will shape research agendas across the sciences for years to come. Therefore it is important to learn how we, as social scientists, can access or generate data that will provide a better understanding of this disease.\n",
    "\n",
    "We will also indulge the research interests of one of the authors of these materials (Diarmuid) by drawing on examples relating to the UK charity sector. Rest assured that this field offers an excellent example of what is possible using web-scraping techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*Collecting data from web pages*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your name and press enter:\n",
      "Diarmuid\n",
      "Hello Diarmuid, enjoy learning more about Python and web-scraping!\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Collecting data from web pages\n",
    "\n",
    "### What is web-scraping?\n",
    "\n",
    "What is web-scraping? It is a computational technique for capturing information stored on a web page. \"Computational\" is the key word, as it is possible to perform this task manually, though that carries considerable disadvantages in terms of accuracy and labour resource.\n",
    "\n",
    "It is generally implemented using a programming script - that is, executable code written in some programming language, though there are software applications that you can use - a previous webinar by our colleague Peter Smyth on the 16 April demonstrated how to use MS Excel to collect data from a website.\n",
    "\n",
    "It is relatively simple to implement using open-source programming languages e.g., Python, R. You do not need to be highly computationally literate, nor write screeds of code: this is a popular and mature computational method, with tons of documentation and examples for you to learn from.\n",
    "\n",
    "### Reasons to engage in web-scraping\n",
    "\n",
    "Websites can be an important source of publicly available information on phenomena of interest - for instance, they are used to store and disseminate files, text, photos, videos, tables etc. However, the data stored on websites are typically not structured or formatted for ease of use by researchers: for example, it may not be possible to perform a bulk download of all the files you need (think of needing the annual accounts of all registered companies in London for your research...), or the information may not even be held in a file and instead spread across paragraphs and tables throughout a web page (or worse, web pages). Luckily, web-scraping provides a means of quickly and accurately capturing and formatting data stored on web pages.\n",
    "\n",
    "Before we delve into writing code to capture data from the web, let's clearly state the logic underpinning the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Logic of web-scraping\n",
    "\n",
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "1. The location (i.e., web address) where the web page can be accessed. For example, the UK Data Service homepage can be accessed via <a href=\"https://ukdataservice.ac.uk/\" target=_blank>https://ukdataservice.ac.uk/</a>.\n",
    "2. The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser.\n",
    "\n",
    "And **do** the following:\n",
    "3. Request the web page using its web address.\n",
    "4. Parse the structure of the web page so your programming language can work with its contents.\n",
    "5. Extract the information we are interested in.\n",
    "6. Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed. \n",
    "\n",
    "For our first example, let's convert the steps above into executable Python code for capturing data about Covid-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Example: Capturing Covid-19 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Worldometer is a website that provides up-to-date statistics on the following domains: the global population; food, water and energy consumption; environmental degradation and many others (known as its Real Time Statistics Project). In its own words:<sup>[1]</sup>\n",
    "> Worldometer is run by an international team of developers, researchers, and volunteers with the goal of making world statistics available in a thought-provoking and time relevant format to a wide audience around the world. Worldometer is owned by Dadax, an independent company. We have no political, governmental, or corporate affiliation.\n",
    "\n",
    "Since the outbreak of Covid-19 it has provided regular daily snapshots on the progress of this disease, both globally and at a country level.\n",
    "\n",
    "[1]: https://www.worldometers.info/about/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Identifying the web address\n",
    "\n",
    "The website can be accessed here: <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>https://www.worldometers.info/coronavirus/</a>\n",
    "\n",
    "Let's work through the steps necessary to collect data about the number of Covid-19 cases, deaths and recoveries globally.\n",
    "\n",
    "First, let's become familiar with this website: click on the link below to view the web page in your browser: <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>https://www.worldometers.info/coronavirus/</a>\n",
    "\n",
    "(Note: it possible to load websites into Python in order to view them, however the website we are interested in doesn't allow this. See the example code below for how it would work for a different website - just remove the quotation marks enclosing the code and run the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://ukdataservice.ac.uk/\", width=\"600\", height=\"650\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "The statistics we need are near the top of the page under the following headings:\n",
    "* Coronavirus Cases:\n",
    "* Deaths:\n",
    "* Recovered:\n",
    "\n",
    "However, we need more information than this in order to scrape the statistics. Websites are written in a langauge called HyperText Markup Language (HTML), which can be understood as follows:<sup>[2]</sup>\n",
    "* HTML describes the structure of a web page\n",
    "* HTML consists of a series of elements\n",
    "* HTML elements tell the browser how to display the content\n",
    "* HTML elements are represented by tags\n",
    "* HTML tags label pieces of content such as \"heading\", \"paragraph\", \"table\", and so on\n",
    "* Browsers do not display the HTML tags, but use them to render the content of the page \n",
    "\n",
    "#### Visually inspecting the underlying HTML code\n",
    "\n",
    "Therefore, what we need are the tags that identify the section of the web page where the statistics are stored. We can discover the tags by examining the *source code* (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select *View Page Source* from the list of options. \n",
    "\n",
    "**TASK**: Try this yourself with the Worldometer web page.\n",
    "\n",
    "The snippet below shows sample source code for the section of the Covid-19 web page we are interested in.\n",
    "\n",
    "<a id=\"source_code_example\"></a>\n",
    "\n",
    "[2]: https://www.w3schools.com/html/html_intro.asp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
    "    <h1>Coronavirus Cases:</h1>\n",
    "    <div class=\"maincounter-number\">\n",
    "        <span style=\"color:#aaa\">252,731        </span>\n",
    "\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center \"><a href=\"#countries\">view by country</a></div>\n",
    "\n",
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
    "    <h1>Deaths:</h1>\n",
    "    <div class=\"maincounter-number\">\n",
    "        <span>10,405</span>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
    "    <h1>Recovered:</h1>\n",
    "    <div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
    "        <span>89,056</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the above example, we can see multiple tags that contain various elements (e.g., text content, other tags). For instance, we can see that the Covid-19 statistics are enclosed in `<span><\\span>` tags, which themselves are located within `<div><\\div>` tags.\n",
    "\n",
    "As you can see, exploring and locating the contents of a web page remains a manual and visual process, and in Brooker's estimation (2020, 252):\n",
    "> Hence, more so than the actual Python, it's the detective work of unpicking the internal structure of a webpage that is probably the most vital skill here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now that we possess the necessary information, let's begin the process of scraping the web page. There is a preliminary step, which is setting up Python with the modules it needs to perform the web-scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported necessary modules\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "\n",
    "import os # module for navigating your machine (e.g., file directories)\n",
    "import requests # module for requesting urls\n",
    "import csv # module for handling csv files\n",
    "import pandas as pd # module for handling data\n",
    "from datetime import datetime # module for working with dates and time\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Modules are additional techniques or functions that are not present when you launch Python. Some do not even come with Python when you download it and must be installed on your machine separately - think of using `ssc install <package>` in Stata, or `install.packages(<package>)` in R. For now just understand that many useful modules need to be imported every time you start a new Python session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Thu, 23 Apr 2020 15:01:50 GMT', 'Content-Type': 'text/html; charset=UTF-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Set-Cookie': '__cfduid=d47acc309ea082de52ae281f26c5a3a331587654110; expires=Sat, 23-May-20 15:01:50 GMT; path=/; domain=.worldometers.info; HttpOnly; SameSite=Lax; Secure', 'X-LiteSpeed-Cache': 'hit', 'Vary': 'Accept-Encoding', 'X-Turbo-Charged-By': 'LiteSpeed', 'CF-Cache-Status': 'DYNAMIC', 'Expect-CT': 'max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"', 'Server': 'cloudflare', 'CF-RAY': '5888764c1ac2e5f4-LHR', 'Content-Encoding': 'gzip', 'alt-svc': 'h3-27=\":443\"; ma=86400, h3-25=\":443\"; ma=86400, h3-24=\":443\"; ma=86400, h3-23=\":443\"; ma=86400', 'cf-request-id': '024928438e0000e5f434938200000001'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the URL where the webpage can be accessed\n",
    "\n",
    "url = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "# Request the webpage from the URL\n",
    "\n",
    "response = requests.get(url, allow_redirects=True) # request the url\n",
    "response.headers\n",
    "#response.status_code # check if page was requested successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, we declare a variable (also known as an 'object' in Python) called `url` that contains the web address of the page we want to request. Next, we use the `get()` method of the `requests` module to request the web page, and in the same line of code, we store the results of the request in a variable called `response`. Finally, we check whether the request was successful by calling on the `status_code` attribute of the `response` variable.\n",
    "\n",
    "Confused? Don't worry, the conventions of Python and using its modules take a bit of getting used to. At this point, just understand that you can store the results of commands in variables, and a variable can have different attributes that can be accessed when needed. Also note that you have a lot of freedom in how you name your variables (subject to certain restrictions - see <a href=\"https://www.python.org/dev/peps/pep-0008/\" target=_blank>here for some guidance</a>).\n",
    "\n",
    "For example, the following would also work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_address = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "scrape_result = requests.get(web_address, allow_redirects=True)\n",
    "scrape_result.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Back to the request:\n",
    "\n",
    "Good, we get a status code of _200_ - this means we successfully requested the web page. <a href=\"https://www.textbook.ds100.org/ch/07/web_http.html\" target=_blank>Lau, Gonzalez and Nolan</a> provide a succinct description of different types of response status codes:\n",
    "\n",
    "* **100s** - Informational: More input is expected from client or server (e.g. 100 Continue, 102 Processing)\n",
    "* **200s** - Success: The client's request was successful (e.g. 200 OK, 202 Accepted)\n",
    "* **300s** - Redirection: Requested URL is located elsewhere; May need user's further action (e.g. 300 Multiple Choices, 301 Moved Permanently)\n",
    "* **400s** - Client Error: Client-side error (e.g. 400 Bad Request, 403 Forbidden, 404 Not Found)\n",
    "* **500s** - Server Error: Server-side error or server is incapable of performing the request (e.g. 500 Internal Server Error, 503 Service Unavailable)\n",
    "\n",
    "For clarity:\n",
    "* **Client**: your machine\n",
    "* **Server**: the machine you are requesting the web page from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You may be wondering exactly what it is we requested: if you were to type the URL (https://www.worldometers.info/coronavirus/) into your browser and hit `enter`, the web page should appear on your screen. This is not the case when we request the URL through Python but rest assured, we have successfully requested the web page. To see the content of our request, we can examine the `text` attribute of the `response` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<!DOCTYPE html>\\n<!--[if IE 8]> <html lang=\"en\" class=\"ie8\"> <![endif]-->\\n<!--[if IE 9]> <html lang=\"en\" class=\"ie9\"> <![endif]-->\\n<!--[if !IE]><!-->\\n<html lang=\"en\">\\n<!--<![endif]-->\\n<head>\\n<meta charset=\"utf-8\">\\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n<title>Coronavirus Update (Live): 2,667,327 Cases and 186,220 Deaths from COVID-19 Virus Pandemic - Worldometer</title>\\n<meta name=\"description\" content=\"Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China. Coronavirus counter with new cases, deaths, and number of tests per 1 Million population. Historical data and info. Daily charts, graphs, news and updates\">\\n\\n<link rel=\"shortcut icon\" href=\"/favicon/favicon.ico\" type=\"image/x-icon\">\\n<link rel=\"apple-touch-icon\" sizes=\"57x57\" href=\"/favicon/apple-icon-57x57.png\">\\n<link rel=\"apple-touch-ic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This shows us a sample of the underlying code (HTML) of the web page we requested. It should be obvious that in its current form, the result of this request will be difficult to work with. This is where the `BeautifulSoup` module comes in handy.\n",
    "\n",
    "(See Appendix A for more examples of how the `requests` module works and what information it returns.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing the web page\n",
    "\n",
    "Now it's time to identify and understand the structure of the web page we requested. We do this by converting the content contained in the `response.text` attribute into a `BeautifulSoup` variable. `BeautifulSoup` is a Python module that provides a systematic way of navigating the elements of a web page and extracting its contents. Let's see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "<!DOCTYPE html>\n",
       "\n",
       "<!--[if IE 8]> <html lang=\"en\" class=\"ie8\"> <![endif]-->\n",
       "<!--[if IE 9]> <html lang=\"en\" class=\"ie9\"> <![endif]-->\n",
       "<!--[if !IE]><!-->\n",
       "<html lang=\"en\">\n",
       "<!--<![endif]-->\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<title>Coronavirus Update (Live): 2,667,327 Cases and 186,220 Deaths from COVID-19 Virus Pandemic - Worldometer</title>\n",
       "<meta content=\"Live statistics and coronavirus news tracking the number of confirmed cases, recovered patients, tests, and death toll due to the COVID-19 coronavirus from Wuhan, China. Coronavirus counter with new cases, deaths, and number of tests per 1 Million population. Historical data and info. Daily charts, graphs, news and updates\" name=\"description\"/>\n",
       "<link href=\"/favicon/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n",
       "<link href=\"/favicon/apple-icon-57x57.png\" rel=\"apple-touch-icon\" sizes=\"57x57\"/>\n",
       "&lt;link rel=\"apple-touch-ic</head></html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the contents of the webpage from the response\n",
    "\n",
    "soup_response = soup(response.text, \"html.parser\") # Parse the text as a Beautiful Soup object\n",
    "soup_sample = soup(response.text[:1000], \"html.parser\") # Parse a sample of the text\n",
    "soup_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "`BeautifulSoup` has taken the unstructured text contained in `response.text` and parsed it as HTML: now we can clearly see the hierarchical structure and tags that comprise a web page's HTML. \n",
    "\n",
    "Note again how we call on a method (`soup()`) from a module (`BeautifulSoup`) and store the results in a variable (`soup_response`).\n",
    "\n",
    "Of course, we've only displayed a sample of the code here for readability. What about the full text contained in `soup_response`: how do we navigate such voluminous results? Thankfully the `BeautifulSoup` module provides some intuitive methods for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**TASK**: view the full contents of the web page stored in the variable `soup_response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extracting information\n",
    "\n",
    "Now that we have parsed the web page, we can use Python to navigate and extract the information of interest. To begin with, let's locate the section of the web page containing the overall Covid-19 statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
       " <h1>Coronavirus Cases:</h1>\n",
       " <div class=\"maincounter-number\">\n",
       " <span style=\"color:#aaa\">2,667,327 </span>\n",
       " </div>\n",
       " </div>,\n",
       " <div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
       " <h1>Deaths:</h1>\n",
       " <div class=\"maincounter-number\">\n",
       " <span>186,220</span>\n",
       " </div>\n",
       " </div>,\n",
       " <div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
       " <h1>Recovered:</h1>\n",
       " <div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
       " <span>730,910</span>\n",
       " </div>\n",
       " </div>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the sections containing the data of interest\n",
    "\n",
    "sections = soup_response.find_all(\"div\", id=\"maincounter-wrap\")\n",
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We used the `find_all()` method to search for all `<div>` tags where the id=\"maincounter-wrap\". And because there is more than one set of tags matching this id, we get a list of results. We can check how many tags match this id by calling on the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can view each element in the list of results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
      "<h1>Recovered:</h1>\n",
      "<div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
      "<span>730,910</span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n",
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
      "<h1>Recovered:</h1>\n",
      "<div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
      "<span>730,910</span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n",
      "--------\n",
      "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
      "<h1>Recovered:</h1>\n",
      "<div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
      "<span>730,910</span>\n",
      "</div>\n",
      "</div>\n",
      "--------\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "for chicken in sections:\n",
    "    print(\"--------\")\n",
    "    print(section)\n",
    "    print(\"--------\")\n",
    "    print(\"\\r\") # print some blank space for better formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are nearing the end of our scrape. The penultimate task is to extract the statistics within the `<span>` tags and store them in some variables. We do this by accessing each item in the _sections_ list using its positional value (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 2667327; deaths: 186220; and recoveries: 730910.\n"
     ]
    }
   ],
   "source": [
    "cases = sections[0].find(\"span\").text.replace(\" \", \"\").replace(\",\", \"\")\n",
    "deaths = sections[1].find(\"span\").text.replace(\",\", \"\")\n",
    "recov = sections[2].find(\"span\").text.replace(\",\", \"\")\n",
    "print(\"Number of cases: {}; deaths: {}; and recoveries: {}.\".format(cases, deaths, recov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The above code performs a couple of operations:\n",
    "* For each item (i.e., set of `<div>` tags) in the list, it finds the `<span>` tags and extracts the text enclosed within them.\n",
    "* We clean the text by removing blank spaces and commas.\n",
    "\n",
    "In this example, referring to an item's positional index works because our list of `<div>` tags stored in the `sections` variable is ordered: the tag containing the number of cases appears before the tag containing the number of deaths, which appears before the tag containing the number of recovered patients.\n",
    "\n",
    "In Python, indexing begins at zero (in R indexing begins at 1). Therefore, the first item in the list is accessed using `sections[0]`, the second using `sections[1]` etc.\n",
    "\n",
    "(To learn more about lists in Python, see Chapter 22 of <a href=\"https://assets.digitalocean.com/books/python/how-to-code-in-python.pdf\" target=_blank>*How to Code in Python 3*</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "The final task is to save the variables to a file that we can use in the future. We'll write to a Comma-Separated Values (CSV) file for this purpose, as it is an open-source, text-based file format that is commonly used for sharing data on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to create folder: already exists\n"
     ]
    }
   ],
   "source": [
    "# Create a downloads folder\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except:\n",
    "    print(\"Unable to create folder: already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The use of \"./\" tells the `os.mkdir()` command that the \"downloads\" folder should be created at the same level of the directory where this notebook is located. So if this notebook was stored in a directory located at \"C:/Users/joebloggs/notebooks\", the `os.mkdir()` command would result in a new folder located at \"C:/Users/joebloggs/notebooks/downloads\".\n",
    "   \n",
    "(Technically the \"./\" is not needed and you could just write `os.mkdir(\"downloads\")` but it's good practice to be explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23\n"
     ]
    }
   ],
   "source": [
    "# Write the results to a CSV file\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\") # get today's date in YYYY-MM-DD format\n",
    "print(date)\n",
    "\n",
    "variables = [\"Cases\", \"Deaths\", \"Recoveries\"] # define variable names for the file\n",
    "outfile = \"./downloads/covid-19-statistics-\" + date + \".csv\" # define a file for writing the results\n",
    "obs = cases, deaths, recov # define an observation (row)\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(variables) # write the variable names to the first row of the file\n",
    "    writer.writerow(obs) # write the observation to the next row in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The code above defines some headers and a name and location for the file which will store the results of the scrape. We then open the file in *write* mode, and write the headers to the first row, and the statistics to subsequent rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid-19-country-statistics-2020-04-23.csv',\n",
       " 'covid-19-statistics-2020-04-23.csv']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check presence of file in \"downloads\" folder\n",
    "\n",
    "os.listdir(\"./downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases,Deaths,Recoveries\n",
      "2667327,186220,730910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(outfile, \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And Voila, we have successfully carried out a web-scrape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Country-level Covid-19 data\n",
    "\n",
    "We will complete our work gathering data on the Covid-19 pandemic by employing the techniques we learned previously to capture country-level statistics. We'll assume some knowledge on your part and thus you'll notice fewer annotations explaining what is happening at each step. As we progress through this example, there are some tasks for you to complete and some questions to be answered also.\n",
    "\n",
    "**TASK**: if you need to, re-acquaint yourself with the <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>Worldometer website</a> - the table is located near the bottom of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's get some the preliminaries out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Request the web page and parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "response.headers\n",
    "soup_response = soup(response.text, \"html.parser\")\n",
    "#\n",
    "# QUESTION: How do you know if the web page was requested successfully?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Find the table containing country-level statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "table = soup_response.find(\"table\", id=\"main_table_countries_today\").find(\"tbody\")\n",
    "rows = table.find_all(\"tr\", style=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Extract the information contained in each row in the table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['World', '2,667,532', '+31,816', '186,252', '+2,186', '730,920', '1,750,360', '57,944', '342', '23.9', '', '', 'All'], ['USA', '850,116', '+1,399', '47,737', '+78', '84,058', '718,321', '14,016', '2,568', '144', '4,330,937', '13,084', 'North America'], ['Spain', '213,024', '+4,635', '22,157', '+440', '89,250', '101,617', '7,705', '4,556', '474', '930,230', '19,896', 'Europe']]\n",
      "\r\n",
      "Number of rows in table: 209\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "global_info = []\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    country_info = [column.text.strip() for column in columns]\n",
    "    global_info.append(country_info)\n",
    "\n",
    "print(global_info[0:3])\n",
    "print(\"\\r\")\n",
    "print(\"Number of rows in table: {}\".format(len(global_info)))\n",
    "print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, we define a blank list to store statistics for each country (`global_info = []`); then for each row in the table, we extract the contents of each column and store the results in a list (`country_info = [column.text.strip() for column in columns])`; finally we add the results for each country to the overall list (`global_info.append(country_info)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We save the results of the scrape to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists\n",
      "./downloads/covid-19-country-statistics-2020-04-23.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except OSError as error:\n",
    "    print(\"Folder already exists\")\n",
    "\n",
    "variables = [\"Country\", \"Total Cases\", \"New Cases\", \"Total Deaths\", \n",
    "            \"New Deaths\", \"Total Recovered\", \"Active Cases\", \n",
    "            \"Serious_Critical\", \"Total Cases Per 1m Pop\", \"Deaths Per 1m Pop\",\n",
    "            \"Total Tests\", \"Tests Per 1m Pop\"]\n",
    "outfile = \"./downloads/covid-19-country-statistics-\" + date + \".csv\"\n",
    "print(outfile)\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(variables)\n",
    "    for country in global_info:\n",
    "        writer.writerow(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we check the file was created; if so we load it into Python and examine its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total Cases</th>\n",
       "      <th>New Cases</th>\n",
       "      <th>Total Deaths</th>\n",
       "      <th>New Deaths</th>\n",
       "      <th>Total Recovered</th>\n",
       "      <th>Active Cases</th>\n",
       "      <th>Serious_Critical</th>\n",
       "      <th>Total Cases Per 1m Pop</th>\n",
       "      <th>Deaths Per 1m Pop</th>\n",
       "      <th>Total Tests</th>\n",
       "      <th>Tests Per 1m Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.4</td>\n",
       "      <td>347,236</td>\n",
       "      <td>12,211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Seychelles</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Fiji</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Jordan</td>\n",
       "      <td>435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>315</td>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>0.7</td>\n",
       "      <td>36,000</td>\n",
       "      <td>3,528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UK</td>\n",
       "      <td>138,078</td>\n",
       "      <td>+4,583</td>\n",
       "      <td>18,738</td>\n",
       "      <td>+638</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118,996</td>\n",
       "      <td>1,559</td>\n",
       "      <td>2,034</td>\n",
       "      <td>276</td>\n",
       "      <td>583,496</td>\n",
       "      <td>8,595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country Total Cases New Cases Total Deaths New Deaths Total Recovered  \\\n",
       "117   Venezuela         298       NaN           10        NaN             122   \n",
       "192  Seychelles          11       NaN          NaN        NaN               6   \n",
       "179        Fiji          18       NaN          NaN        NaN               8   \n",
       "105      Jordan         435       NaN            7        NaN             315   \n",
       "6            UK     138,078    +4,583       18,738       +638             NaN   \n",
       "\n",
       "    Active Cases Serious_Critical Total Cases Per 1m Pop Deaths Per 1m Pop  \\\n",
       "117          166                4                     10               0.4   \n",
       "192            5              NaN                    112               NaN   \n",
       "179           10              NaN                     20               NaN   \n",
       "105          113                5                     43               0.7   \n",
       "6        118,996            1,559                  2,034               276   \n",
       "\n",
       "    Total Tests Tests Per 1m Pop  \n",
       "117     347,236           12,211  \n",
       "192         NaN              NaN  \n",
       "179         NaN              NaN  \n",
       "105      36,000            3,528  \n",
       "6       583,496            8,595  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(outfile, encoding = \"ISO-8859-1\", index_col=False)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total Cases</th>\n",
       "      <th>New Cases</th>\n",
       "      <th>Total Deaths</th>\n",
       "      <th>New Deaths</th>\n",
       "      <th>Total Recovered</th>\n",
       "      <th>Active Cases</th>\n",
       "      <th>Serious_Critical</th>\n",
       "      <th>Total Cases Per 1m Pop</th>\n",
       "      <th>Deaths Per 1m Pop</th>\n",
       "      <th>Total Tests</th>\n",
       "      <th>Tests Per 1m Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>16,671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,233</td>\n",
       "      <td>6,669</td>\n",
       "      <td>147</td>\n",
       "      <td>3,376</td>\n",
       "      <td>156</td>\n",
       "      <td>111,584</td>\n",
       "      <td>22,598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country Total Cases New Cases Total Deaths New Deaths Total Recovered  \\\n",
       "19  Ireland      16,671       NaN          769        NaN           9,233   \n",
       "\n",
       "   Active Cases Serious_Critical Total Cases Per 1m Pop Deaths Per 1m Pop  \\\n",
       "19        6,669              147                  3,376               156   \n",
       "\n",
       "   Total Tests Tests Per 1m Pop  \n",
       "19     111,584           22,598  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Country\"]==\"Ireland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: change the code below to view the records for Ireland, Spain, and Kuwait.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total Cases</th>\n",
       "      <th>New Cases</th>\n",
       "      <th>Total Deaths</th>\n",
       "      <th>New Deaths</th>\n",
       "      <th>Total Recovered</th>\n",
       "      <th>Active Cases</th>\n",
       "      <th>Serious_Critical</th>\n",
       "      <th>Total Cases Per 1m Pop</th>\n",
       "      <th>Deaths Per 1m Pop</th>\n",
       "      <th>Total Tests</th>\n",
       "      <th>Tests Per 1m Pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Kuwait</td>\n",
       "      <td>2,399</td>\n",
       "      <td>+151</td>\n",
       "      <td>14</td>\n",
       "      <td>+1</td>\n",
       "      <td>498</td>\n",
       "      <td>1,887</td>\n",
       "      <td>55</td>\n",
       "      <td>562</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country Total Cases New Cases Total Deaths New Deaths Total Recovered  \\\n",
       "58  Kuwait       2,399      +151           14         +1             498   \n",
       "\n",
       "   Active Cases Serious_Critical Total Cases Per 1m Pop Deaths Per 1m Pop  \\\n",
       "58        1,887               55                    562                 3   \n",
       "\n",
       "   Total Tests Tests Per 1m Pop  \n",
       "58         NaN              NaN  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Country\"]==\"Kuwait\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Concluding remarks on Covid-19 data\n",
    "\n",
    "The Covid-19 pandemic is a seismic public health crisis that will dominate our lives for the foreseeable future. The example code above is not a craven attempt to provide some topicality to these materials, nor is it simply a particularly good example for learning web-scraping techniques. There are real opportunities for social scientists to capture and analyse data on this phenomenon, starting with the core figures provided through the <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>Worldometer website</a>.\n",
    "\n",
    "You may also be interested in the publicly available data repository provided by Johns Hopkins University Center for Systems Science and Engineering (JHU CSSE): <a href=\"https://github.com/CSSEGISandData/COVID-19\" target=_blank>https://github.com/CSSEGISandData/COVID-19</a>. Updated daily, this resource provides CSV (Comma Separated Values) files of global Covid-19 statistics (e.g., country-level time series), as well as PDF copies of the World Health Organisation's situation reports.\n",
    "\n",
    "At a UK level, the NHS releases data about Covid-19 symptoms reported through its NHS Pathways and 111 online platforms: <a href=\"https://digital.nhs.uk/data-and-information/publications/statistical/mi-potential-covid-19-symptoms-reported-through-nhs-pathways-and-111-online/latest\" target=_blank>NHS Open Data</a>. Data on reported cases is also provided by Public Health England (PHE): <a href=\"https://www.gov.uk/government/publications/covid-19-track-coronavirus-cases\" target=_blank>COVID-19: track coronavirus cases</a>. Many of these datasets are available as openly available as CSV files - you can learn how to download files in the [charity data example](#section_9_3).\n",
    "\n",
    "Finally, the Office for National Statistics (ONS) provides data and experimental indicators of social like in the UK under Covid-19: <a href=\"https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/conditionsanddiseases\" target=_blank>Coronavirus (COVID-19)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A social research example: charity data\n",
    "\n",
    "The example contained in Appendix B introduces slightly more complicated web pages, and techniques for handling exceptions, downloading files and more. If you feel comfortable with what you've learned so far then we highly recommend completing this lesson; if not take some more time to digest the Covid-19 example and return to it at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Value, limitations and ethics\n",
    "\n",
    "Computational methods for collecting data from the web are an increasingly important component of a social scientist's toolkit. They enable individuals to collect and reshape data - qualitative and quantitative - that otherwise would be inaccessible for research purposes. Thus far, this notebook has focused on the logic and practice of web-scraping, however it is crucial we reflect critically on its value, limitations and ethical implications for social science purposes.\n",
    "\n",
    "### Value\n",
    "\n",
    "* Web-scraping is a mature computational method, with lots of established packages (e.g., `requests` and `BeautifulSoup` in Python), examples and help available. As a result the learning curve is not as steep as with other methods, and it is possible for a beginner to create and execute a functioning web-scraping script in a matter of hours.\n",
    "* Using computational, rather than manual, methods provides the ability to schedule or automate your data collection activities. For instance, you could schedule the Covid-19 code to execute at a set time every day.\n",
    "* The richness of some of the information and data stored on web pages is a point worth repeating. Many public, private and charitable institutions use their web sites to release and regularly update information of value to social scientists. Getting a handle on the volume, variety and velocity of this information is extremely challenging without the use of computational methods.\n",
    "* Computational methods not only enable accurate, real-time and reliable data collection, they also enable the reshaping of data into familiar formats (e.g., a CSV file, a database, a text document). While Python and HTML might be unfamiliar, the data that is returned through web-scraping can be formatted in such a way as to be compatible with your usual analytical methods (e.g., regression modelling, content analysis) and software applications (e.g., Stata, NVivo). In fact, we would go as far to say that computational methods are particularly valuable to social scientists from a data collection and processing perspective, and you can achieve much without ever engaging in \"big data analytics\" (e.g., machine learning, neural networks, natural language processing).\n",
    "\n",
    "### Limitations\n",
    "\n",
    "* Web-scraping may contravene the Terms of Service (ToS) of a website.  Much like open datasets will have a licence stipulating how the data may be used, information stored on the web can also come with restrictions on use. For example, the <a href=\"https://www.worldometers.info/licensing/faq/\" target=_blank>Worldometer Covid-19 data</a> that we scraped cannot be used without their permission, even though the <a href=\"https://www.worldometers.info/disclaimer/\" target=_blank>ToS</a> do not expressly prohibit web-scraping. In contrast, the beneficiary data provided by the Charity Commission for England and Wales (see Appendix B) is available under the <a href=\"https://www.nationalarchives.gov.uk/doc/open-government-licence/version/2/\" target=_blank>Open Government Licence (OGL) Version 2</a>, which permits copying, publishing, distribution and transmission. <br>So even in instances where scraping data is not prohibited, you may not be able to use it without seeking permission from the data owner. The safest approach is to seek permission in advance of conducting a web-scrape, especially if you intend to build a working relationship with the data owner - do not rely on the argument that the information is publicly available to begin with. In certain cases it may be easier to manually record or collect the data you are interested in.\n",
    "* This brings us to a related point concerning the legal basis for collecting and using data from websites. In the UK there is no specific law prohibiting web-scraping or the use of data obtained via this method; however there are other laws which impinge on this activity. Copyright or intellectual property law may prohibit what information, if any, may be scraped from a website (<a href=\"http://copyrightblog.kluweriplaw.com/2015/01/26/ryanair-ltd-v-pr-aviation-bv-contracts-rights-and-users-in-a-low-cost-database-law/?doing_wp_cron=1586338639.9420111179351806640625\" target=_blank>see this example</a>). <br>Data protection laws, such as the General Data Protection Regulations (GDPR), also influence whether and how you collect data about individuals. This means you take responsibility for processing personal data, even if its publicly available. This is a critical and detailed area of data-driven activities, and we encourage you to consult relevant guidance (see *Further reading and resources* section).\n",
    "* Web pages are frequently updated, therefore changes to their structure can break your code e.g., the URL for a file may change, or the table element now has a different id or was moved to a different web page. It can be a lot of work maintaining your code, especially if you make it available for use by others.\n",
    "* Some websites may be advanced enough that they throttle or block scraping of their contents. For example, they may \"blacklist\" (ban) your IP address - your computer's unique id on the internet - from making requests to its server.\n",
    "* Web-scraping, and computational social science in general, is dependent on your computing setup. For example, you may not possess administrative rights on your machine, preventing you from scheduling your script to run on a regular basis (e.g., your computer automatically goes to sleep after a set period of time and you cannot change this setting). There are ways around this and you do not need a high performance computing setup to scrape data from the web, but it is worth keeping in mind nonetheless.\n",
    "\n",
    "See the *Further reading and resources* section for useful articles exploring many of these issues.\n",
    "\n",
    "### Ethical considerations\n",
    "\n",
    "For the purposes of this discussion, we will assume you have sought and received ethical approval for a piece of research through the usual institutional processes: you've already considered consent, harm to researcher and participant, data security and curation etc. Instead, we will focus on a major ethical implication specific to web-scraping: the impact on the data owner's website. Each request you make to a website consumes computational resources, on your end and theirs: the server (i.e., machine) hosting the website must use some of its processing power and bandwidth to respond to the request. Web-scraping, especially frequently scheduled scripts, can overload a server by making too many requests, causing the website to crash. Individuals and organisations may rely on a website for vital and timely information, and causing it to crash could carry significant real-world implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Web-scraping is a simple yet powerful computational method for collecting data of value for social science research. It provides a relatively gentle introduction to using programming languages, also. However, \"with great power comes great responsibility\" (sorry). Web-scraping takes you into the realm of data protection, website Terms of Service (ToS), and many murky ethical issues. Wielded sensibly and sensitively, web-scraping is a valuable and exciting social science research method. \n",
    "\n",
    "Good luck on your data-driven travels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "Barba, Lorena A. et al. (2019). *Teaching and Learning with Jupyter*. <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>https://jupyter4edu.github.io/jupyter-edu-book/</a>.\n",
    "\n",
    "Brooker, P. (2020). *Programming with Python for Social Scientists*. London: SAGE Publications Ltd.\n",
    "\n",
    "Lau, S., Gonzalez, J., & Nolan, D. (n.d.). *Principles and Techniques of Data Science*. https://www.textbook.ds100.org\n",
    "\n",
    "Tagliaferri, L. (n.d.). *How to Code in Python 3*. https://assets.digitalocean.com/books/python/how-to-code-in-python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Further reading and resources\n",
    "\n",
    "We publish a list of useful books, papers, websites and other resources on our web-scraping Github repository: <a href=\"https://github.com/UKDataServiceOpen/web-scraping/tree/master/reading-list/\" target=_blank>[Reading list]</a>\n",
    "\n",
    "The help documentation for the `requests` and `BeautifulSoup` modules is refreshingly readable and useful:\n",
    "* <a href=\"https://requests.readthedocs.io/en/master/\" target=_blank>`requests`</a>\n",
    "* <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=_blank>`BeautifulSoup`</a> \n",
    "\n",
    "You may also be interested in the following articles specifically relating to web-scraping:\n",
    "* <a href=\"https://ico.org.uk/for-organisations/guide-to-data-protection\" target=_blank>Guide to Data Protection</a>\n",
    "* <a href=\"https://ocean.sagepub.com/blog/collecting-social-media-data-for-research\" target=_blank>Collecting social media data for research</a>\n",
    "* <a href=\"https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/\" target=_blank>Web Scraping and Crawling Are Perfectly Legal, Right?</a>\n",
    "* <a href=\"https://parissmith.co.uk/blog/web-crawling-screen-scraping-legal-position/\" target=_blank>Web Crawling and Screen Scraping  the Legal Position</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Appendix A - Requesting URLs\n",
    "\n",
    "In Python we've made use of the excellent `requests` module. By calling the `requests.get()` method, we mimic the manual process of launching a web browser and visiting a website. The `requests` module achieves this by placing a _request_ to the server hosting the website (e.g., show me the contents of the website), and handling the _response_ that is returned (e.g., the contents of the website and some metadata about the request). This _request-response_ protocol is known as HTTP (HyperText Transfer Protocol); HTTP allows computers to communicate with each other over the internet - you can learn more about it at <a href=\"https://www.w3schools.com/whatis/whatis_http.asp\" target=_blank>W3 Schools</a>.\n",
    "\n",
    "Run the code below to learn more about the data and metadata returned by `requests.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://httpbin.org/html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "print(\"1. {}\".format(response)) # returns the object type (i.e. a response) and status code\n",
    "print(\"\\r\")\n",
    "\n",
    "print(\"2. {}\".format(response.headers)) # returns a dictionary of response headers\n",
    "print(\"\\r\")\n",
    "\n",
    "print(\"3. {}\".format(response.headers[\"Date\"])) # return a particular header\n",
    "print(\"\\r\")\n",
    "\n",
    "print(\"4. {}\".format(response.request)) # returns the request object that requested this response\n",
    "print(\"\\r\")\n",
    "\n",
    "print(\"5. {}\".format(response.url)) # returns the URL of the response\n",
    "print(\"\\r\")\n",
    "\n",
    "#print(response.text) # returns the text contained in the response (i.e. the paragraphs, headers etc of the web page)\n",
    "#print(response.content) # returns the content of the response (i.e. the HTML contents of the web page)\n",
    "\n",
    "# Visit https://www.w3schools.com/python/ref_requests_response.asp for a full list of what is returned by the server\n",
    "# in response to a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Appendix B - Capturing charity data\n",
    "\n",
    "We conclude this notebook with a social research-oriented example: who do UK charities claim to support through their activities?\n",
    "\n",
    "Let's decompose the data collection process into its constituent parts:\n",
    "1. Extract a list of charity numbers from an administrative dataset.\n",
    "2. For each of these charity numbers, request and parse its web page from the Charity Commission for England and Wales (CCEW) website.\n",
    "3. Extract the list of beneficiary groups - listed under the header \"Who the charity helps\".\n",
    "4. Write the results to a CSV file.\n",
    "5. Download a charity's set of annual accounts to capture additional information (e.g., how the organisation helped its beneficiaries).\n",
    "\n",
    "When developing a more complicated script, it's best to test it with a simple/limited example first. In our case, let's try to extract the beneficiary groups for a randomly selected charity before doing so for a larger sample of organisations.\n",
    "\n",
    "First, let's take care of importing modules and other preliminary tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "try:\n",
    "    import csv # module for handling csv files\n",
    "    import requests # module for requesting urls\n",
    "    import os # module for performing operating system tasks\n",
    "    import pandas as pd # module for working with datasets\n",
    "    import random # module for generating pseudo-random numbers\n",
    "    from datetime import datetime # module for working with dates and time\n",
    "    from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "    print(\"Successfully imported modules\")\n",
    "except:\n",
    "    print(\"Did not import one or more modules!\")  \n",
    "    \n",
    "# Create folder and file\n",
    "try:\n",
    "    os.mkdir('./downloads')\n",
    "except OSError as error:\n",
    "    print(\"Folder already exists\")\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\") # get today's date\n",
    "\n",
    "outfile = \"./downloads/charity-beneficiaries-\" + date + \".csv\" # CSV file for saving results of scrape\n",
    "outfile # view path to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Extract list of charity numbers\n",
    "\n",
    "Good, we've successfully imported the modules we need and define a location and file for storing the results of the scrape. Now let us import the raw data file containing a list of charity numbers and take a random sample of these for testing our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Import list of charities from raw data\n",
    "#\n",
    "# We'll use an older copy of the Register of Charities provided by the Charity Commission for England and Wales (CCEW).\n",
    "# This file contains information on all registered charities in England and Wales (c. 160,000 organisations).\n",
    "#\n",
    "\n",
    "charreg_file = \"./data/extract_main_charity.csv\" # location of file\n",
    "charreg = pd.read_csv(charreg_file, encoding = \"ISO-8859-1\", index_col=False) # import file\n",
    "\n",
    "\n",
    "# Explore dataset characteristics\n",
    "\n",
    "print(\"{} observations in the dataset\".format(len(charreg))) # print the number of rows in the dataset\n",
    "print(charreg.shape) # print the number of rows and columns in the dataset\n",
    "print(charreg.columns) # print the names of the columns in the dataset\n",
    "charreg.sample(5) # view 5 randomly chosen observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Extract charity numbers and take a random sample of these\n",
    "\n",
    "charreg[\"regno\"] = charreg[\"regno\"].fillna(0).astype(int) # convert missing values to \"0\" and remove decimal places\n",
    "regno_list = charreg.regno.values.tolist() # extract values in \"regno\" column and place in a list\n",
    "print(regno_list[0:5]) # return first five charity numbers in the list\n",
    "\n",
    "random.seed(2) # ensure the random sample is consistent every time the code is run\n",
    "regno_rsamp = random.sample(regno_list, 10) # Draw a random sample of charity numbers from the list\n",
    "print(regno_rsamp)\n",
    "regno = regno_rsamp[2] # select third number in list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Requesting and parsing charity details\n",
    "\n",
    "Now we need to use the ```requests``` and ```BeautifulSoup``` modules to request and parse each charity's web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Request web pages of charities\n",
    "\n",
    "print(\"Charity number: {}\".format(str(regno)))\n",
    "print(\"\\r\")\n",
    "\n",
    "url = \"https://beta.charitycommission.gov.uk/charity-details/?regId=\" + str(regno) + \"&subId=0\"\n",
    "print(\"Requesting URL: {}\".format(url))\n",
    "print(\"\\r\")\n",
    "#\n",
    "# For now we will just request the details of the first charity number\n",
    "# in the list of random samples.\n",
    "#\n",
    "\n",
    "response = requests.get(url, allow_redirects=True) # request the url\n",
    "print(response.status_code, \" | \", response.headers) # print the metadata behind the request to see if it was successful\n",
    "print(\"\\r\")\n",
    "\n",
    "\n",
    "# Parse the contents of the web page\n",
    "\n",
    "html_response = response.text # get the text elements of the page\n",
    "soup_response = soup(html_response, 'html.parser') # parse the text as a BeautifulSoup object\n",
    "print(type(soup_response)) # return object type (just to confirm it is a BeautifulSoup object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Great, we've successfully requested the web page and parsed it as a BeautifulSoup object; this is important as the ```BeautifulSoup``` module has a number of useful functions (e.g. ```find()```, ```find_all()```) that only work on this object type.\n",
    "\n",
    "Now we come to the scrape - finding and extracting the list of beneficiaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Extracting beneficiary group information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the list of beneficiary groups\n",
    "#\n",
    "# The informtion we need is contained in a set of <div></div> tags identified by its \n",
    "# \"class=pcg-charity-details__block col-lg-4\" attribute. Unfortunately this is not a unique id, so we\n",
    "# need to find all instances where \"class=pcg-charity-details__block col-lg-4\" and filter to the correct set of <div></div>.\n",
    "#\n",
    "\n",
    "sections = soup_response.find_all(\"div\", class_=\"pcg-charity-details__block col-lg-4\") # find all <div> tags where class equals stated value\n",
    "print(len(sections)) # return how many tags were found matching the soup_response.find_all() expression above\n",
    "print(\"\\r\")\n",
    "\n",
    "\n",
    "# Find beneficiary section in list of sections\n",
    "\n",
    "searchterm = \"Who the charity helps\" # search term identifying section containing list of beneficiaries\n",
    "for section in sections: # for each section contained in the sections list:\n",
    "    if searchterm in str(section): # if the search term exists in the section\n",
    "        print(\"Index (position) of relevant section: {}\".format(sections.index(section))) # return a message saying we found the correct section\n",
    "        benlocation = sections.index(section) # store the location in the list of the correct section\n",
    "        print(\"\\r\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "bensection = sections[benlocation] # create a new object containing the correct section\n",
    "print(bensection) # return the contents of the section\n",
    "print(\"\\r\")\n",
    "\n",
    "\n",
    "# Extract beneficiary groups from correct section #\n",
    "\n",
    "benlist = [] # define a blank list for storing results of scrape\n",
    "\n",
    "for item in bensection.find_all('li'): # for each <li> tag in the beneficaries section\n",
    "    charid = str(regno) # store the unique id of the charity\n",
    "    beneficiary = str(item.text) # store the text contained within the <li></li> tags\n",
    "    \n",
    "    observation = [charid, beneficiary] # create a list containing charity id, name and beneficiary group\n",
    "    benlist.append(observation) # add the observations to the original list\n",
    "\n",
    "print(benlist) # now we have a list of beneficiary groups (long format) for Oxfam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's unpick the logic of the code above:\n",
    "1. We know the list of beneficiaries is contained in a section (`<div>`) where *class_=\"pcg-charity-details__block col-lg-4\"*.\n",
    "2. We find all sections where the _class_ attribute equals \"pcg-charity-details__block col-lg-4\", and navigate to the correct one by evaluating whether it contains a relevant string (\"Who the charity helps\"). This process revealed that the list of beneficiaries was contained in the fifth section (remember: lists begin at position 0, so 4 identifies the fifth element of a list). If we knew that the list of beneficiaries was always contained in the fifth section we wouldn't need the use of a search term, but this way is more robust to deviations in the structure and content of each charity's web page.\n",
    "3. Once we identify the correct section, we extract all of the text contained in the `<li></li>` tags and store the results in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Save results to a file\n",
    "\n",
    "Our final task is to write the results of the scrape (benlist) to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Write the results to a CSV file #\n",
    "\n",
    "variables = [\"Charity Number\", \"Beneficiary Group\"] # define variable names for the file\n",
    "\n",
    "with open(outfile, 'w', newline='') as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(variables) # write the variable names to the first row of the file\n",
    "    for row in benlist: # for every observation in the list\n",
    "        writer.writerow(row) # write the observation to a row in the file\n",
    "\n",
    "print(\"----------------------------\")   \n",
    "print(\"\\r\")\n",
    "print(\"Successfully saved the file\")\n",
    "print(\"\\r\")\n",
    "print(\"Contents of downloads folder: {}\".format(os.listdir(\"./downloads\"))) # list the contents of the 'downloads' directory\n",
    "print(\"\\r\")\n",
    "\n",
    "\n",
    "# Open file to check scrape worked #\n",
    "\n",
    "with open(outfile, \"r\") as f: # with the file open in \"read\" mode, and giving it a shorter name (f)\n",
    "    print(f.read()) # print the contents of the file\n",
    "\n",
    "print(\"----------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And that completes the scrape! It seems to work as intended for this one charity but it would be good to check its robustness with a larger sample. The final section in this appendix contains a more detailed script which:\n",
    "* Executes the script for a random sample of 1000 charities.\n",
    "* Creates a log file to record metadata about the scrape (e.g., how long it takes to execute, which urls it requested).\n",
    "\n",
    "Let's conclude by using the `requests` module to download the most recent annual report for this charity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Downloading files\n",
    "\n",
    "As a final task, we will attempt to download the 2018/19 annual accounts for our charity (regno: 211535), which are available on the Charity Commission for England and Wales (CCEW) public website. Our task is simplified by knowing the URL through which the file can be accessed: <a href=\"http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20190331_E_C.PDF\" target=_blank>http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20190331_E_C.PDF</a>.\n",
    "\n",
    "Let's see how we can use Python to achieve this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define the URL where the file can be downloaded\n",
    "\n",
    "accounts = \"http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20190331_E_C.PDF\"\n",
    "\n",
    "\n",
    "# Define where the file should be downloaded to\n",
    "\n",
    "outfile = \"./downloads/annual-accounts-211535-2019.pdf\"\n",
    "\n",
    "\n",
    "# Request the file from the URL \n",
    "\n",
    "response = requests.get(accounts, allow_redirects=True) # request the url and allow redirects if needed\n",
    "print(response.status_code, \" | \", response.headers) # print the metadata behind the request to see if it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(type(response.content)) # reveal what type of Python object the response is; 'bytes' is a file\n",
    "\n",
    "# Save the PDF in the location we defined earlier (i.e. 'outfile') #\n",
    "\n",
    "with open(outfile, \"wb\") as f: # with the file open in \"write binary\" mode, and giving it a shorter name (f)\n",
    "    f.write(response.content) # write the contents (i.e. PDF) of the request we made to the file\n",
    "\n",
    "print(\"Successfully saved the file\")\n",
    "#\n",
    "# It may seem strange to open the file before placing the contents of\n",
    "# the PDF in it, but think of it like opening a blank spreadsheet and \n",
    "# copying-and-pasting a table into it. \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "How can we tell it worked? First, we wrote a simple ```print``` command that would only execute if the code preceding it worked correctly. We could also ask Python to list the contents of the _downloads_ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(\"./downloads\") # list the contents of the 'downloads' directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Finally, we can open the PDF in our Jupyter notebook to view its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(outfile, width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Congratulations! You've learned how to download a file from a URL using simple and efficient Python code. Why didn't we just open a browser and peform this task manually? Well, a programming script has the following advantages:\n",
    "* Quicker (once the script has been written) \n",
    "* Reproducible\n",
    "* Automatable \n",
    "* Less prone to error\n",
    "\n",
    "The advantages are obvious once we expand our data collection efforts to more units of analysis: there are c. 160,000 registered charities in England and Wales, many of which must submit accounts going back at least five years. I certainly don't fancy performing this task manually or subjecting a research assistant to it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**EXERCISE**: Adapt the script below to download all of the annual accounts available for this charity from the regulator's website: <br><br><a href=\"https://beta.charitycommission.gov.uk/charity-details/?regid=211535&subid=0\" target=_blank>https://beta.charitycommission.gov.uk/charity-details/?regid=211535&subid=0</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Download charity accounts\n",
    "\n",
    "# Create a list of URLs where the files can be downloaded from\n",
    "\n",
    "accounts = [\"http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20161231_E_C.PDF\",\n",
    "            \"http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20151231_E_C.PDF\",\n",
    "            \"http://apps.charitycommission.gov.uk/Accounts/Ends35/0000211535_AC_20141231_E_C.PDF\"]\n",
    "\n",
    "year = 2016\n",
    "for url in accounts:\n",
    "    \n",
    "    outfile = \"./downloads/annual-accounts-211535-\" + str(year) + \".pdf\" # folder and file name for downloaded file\n",
    "    \n",
    "    # TASK: INSERT CODE HERE TO REQUEST THE URL\n",
    "    print(response.status_code, \" | \", response.headers) # print the metadata behind the request to see if it was successful\n",
    "    \n",
    "\n",
    "    with open(outfile, \"wb\") as f: # with the file open in \"write binary\" mode, and giving it a shorter name (f)\n",
    "        # TASK: INSERT CODE HERE TO WRITE CONTENTS OF THE PDF TO THE FILE\n",
    "\n",
    "    year -=1 # decrement the year variable by one\n",
    "    \n",
    "# QUESTION: Why do we use a \"year\" variable for naming the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# TASK: INSERT CODE HERE TO VIEW THE 2016 ACCOUNTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Extensions\n",
    "\n",
    "This section contains a more detailed charity beneficiary scrape script. It performs the scrape for a larger sample of charities, is robust to instances where beneficiary information is not available, and captures metadata about the scrape also. It is not perfect and I'm sure you can think of improvements like the following:\n",
    "* Currently, the script scrapes information for each charity and stores this in an expanding list, which is only written to the CSV file once the scrape is complete. This could be improved by writing to the CSV file **within** the loop; not only does this ensure we capture some records in the event the script stops executing, it also prevents the list becoming so large it uses up too much computer memory.\n",
    "* What happens if the web page cannot be requested? At the moment the script would break, as there is a difference between an unsuccessful request (no web page appears) and a successful one but the beneficiary information is simply not available (which the script is able to handle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {},
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Title: Scraping charity beneficiaries\n",
    "## Created: 01/04/2020\n",
    "## Creater: Diarmuid McDonnell, University of Manchester\n",
    "\n",
    "# 0. Preliminaries\n",
    "\n",
    "# Import modules\n",
    "\n",
    "try:\n",
    "    import csv # module for handling csv files\n",
    "    import requests # module for requesting urls\n",
    "    import os # module for performing operating system tasks\n",
    "    import pandas as pd # module for working with datasets\n",
    "    import random # module for generating pseudo-random numbers\n",
    "    from datetime import datetime # module for working with dates and time\n",
    "    from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "    print(\"Successfully imported modules\")\n",
    "except:\n",
    "    print(\"Did not import one or more modules!\")  \n",
    "    \n",
    "\n",
    "# Define files to save results of scrape\n",
    "\n",
    "try:\n",
    "    os.mkdir('./downloads')\n",
    "except OSError as error:\n",
    "    print(\"Folder already exists\")\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\") # get today's date\n",
    "\n",
    "outfile = \"./downloads/charity-beneficiaries-\" + date + \".csv\" # CSV file for saving results of scrape\n",
    "logfile = \"./downloads/charity-beneficiaries-log-\" + date + \".csv\" # log file for saving metadata of scrape\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "    \n",
    "# 1. Import list of charities from raw data\n",
    "#\n",
    "# We'll use an older copy of the Register of Charities provided by the Charity Commission for England and Wales (CCEW).\n",
    "# This file contains information on all registered charities in England and Wales (c. 160,000 organisations).\n",
    "#\n",
    "\n",
    "charreg_file = \"./data/extract_main_charity.csv\" # location of file\n",
    "charreg = pd.read_csv(charreg_file, encoding = \"ISO-8859-1\", index_col=False) # import file\n",
    "\n",
    "\n",
    "# Extract charity numbers and take a random sample of these\n",
    "\n",
    "charreg[\"regno\"] = charreg[\"regno\"].fillna(0).astype(int) # convert missing values to \"0\" and remove decimal places\n",
    "regno_list = charreg.regno.values.tolist() # extract values in \"regno\" column and place in a list\n",
    "print(regno_list[0:5]) # return first five charity numbers in the list\n",
    "\n",
    "regno_rsamp = random.sample(regno_list, 1000) # Draw a random sample of charity numbers from the list\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "    \n",
    "# 2. Request web pages of charities\n",
    "\n",
    "benlist = [] # define a blank list for storing results of scrape\n",
    "loglist = [] # define a blank list for storing metadata of scrape\n",
    "\n",
    "for regno in regno_rsamp:\n",
    "    \n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting scrape of charity number: {}\".format(str(regno)))\n",
    "    print(\"\\r\")\n",
    "    starttime = datetime.now() # Track how long it takes to capture information for each charity\n",
    "    \n",
    "    charurl = \"https://beta.charitycommission.gov.uk/charity-details/?regId=\" + str(regno) + \"&subId=0\" \n",
    "    # define the URL of for a given charity's web page\n",
    "\n",
    "    response = requests.get(charurl, allow_redirects=True) # request the url\n",
    "    print(response.status_code, \" | \", response.headers) # print the metadata behind the request to see if it was successful\n",
    "    print(\"\\r\")\n",
    "\n",
    "    html_response = response.text # Get the text elements of the page\n",
    "    soup_response = soup(html_response, 'html.parser') # Parse the text as a BeautifulSoup object\n",
    "\n",
    "    \n",
    "    # 3. Find beneficiary section in list of sections\n",
    "    \n",
    "    if not soup_response.find('span', {\"class\": \"pcg-page-path__status pcg-page-path__status--removed\"}): # If the charity isn't removed from the Register then proceed with scraping beneficiary info\n",
    "        sections = soup_response.find_all(\"div\", class_=\"pcg-charity-details__block col-lg-4\") # find all <div> tags where class equals stated value\n",
    "        #print(len(sections)) # return how many tags were found matching the soup_response.find_all() expression above\n",
    "\n",
    "        searchterm = \"Who the charity helps\" # search term identifying section containing list of beneficiaries\n",
    "        for section in sections: # for each section contained in the sections list:\n",
    "            if searchterm in str(section): # if the search term exists in the section\n",
    "                #print(\"Index (position) of relevant section: {}\".format(sections.index(section))) # return a message saying we found the correct section\n",
    "                benlocation = sections.index(section) # store the location in the list of the correct section\n",
    "                #print(\"\\r\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        bensection = sections[benlocation] # create a new object containing the correct section\n",
    "\n",
    "        for item in bensection.find_all('li'): # for each <li> tag in the beneficaries section\n",
    "            charid = str(regno) # store the unique id of the charity\n",
    "            beneficiary = str(item.text) # store the text contained within the <li></li> tags\n",
    "            observation = [charid, beneficiary] # create a list containing charity id, name and beneficiary group\n",
    "            benlist.append(observation) # add the observations to the original list\n",
    "\n",
    "        runtime = datetime.now() - starttime # calculate how long the scrape took for this charity\n",
    "        scraped = \"Yes\"\n",
    "        logobs = [runtime, charid, charurl, response.status_code, scraped]\n",
    "        loglist.append(logobs)\n",
    "        \n",
    "        print(\"Finished scraping beneficiaries for: {}\".format(str(regno)))\n",
    "        print(\"--------------------------------------------------------\")\n",
    "        \n",
    "    else: # charity is no longer registered, thus no beneficiary info available\n",
    "        charid = str(regno)\n",
    "        runtime = datetime.now() - starttime # calculate how long the scrape took for this charity\n",
    "        scraped = \"No\"\n",
    "        logobs = [runtime, charid, charurl, response.status_code, scraped]\n",
    "        loglist.append(logobs)\n",
    "        \n",
    "        print(\"Could not scrape beneficiaries for: {}\".format(str(regno)))\n",
    "        print(\"--------------------------------------------------------\")\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "\n",
    "# 4. Write the results to a CSV file \n",
    "\n",
    "variables = [\"Charity Number\", \"Organisation Name\", \"Beneficiary Group\"] # define variable names for the results file\n",
    "logheaders = [\"Timestamp\", \"Charity Number\", \"URL\", \"Status Code\", \"Scraped\"] # define variable names for the log file\n",
    "\n",
    "\n",
    "# Write the results #\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(variables) # write the variable names to the first row of the file\n",
    "    for row in benlist: # for every observation in the list\n",
    "        writer.writerow(row) # write the observation to a row in the file\n",
    "        \n",
    "        \n",
    "# Write the log file #\n",
    "\n",
    "with open(logfile, \"w\", newline=\"\") as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(logheaders) # write the variable names to the first row of the file\n",
    "    for row in loglist: # for every observation in the list\n",
    "        writer.writerow(row) # write the observation to a row in the file        \n",
    "\n",
    "        \n",
    "print(\"----------------------------\")   \n",
    "print(\"\\r\")\n",
    "print(\"Successfully saved the files\")\n",
    "print(os.listdir(\"./downloads\")) # list the contents of the 'downloads' directory\n",
    "print(\"\\r\")\n",
    "print(\"Script complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "-- END OF FILE --"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "472px",
    "width": "285px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
